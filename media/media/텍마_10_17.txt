[벡터 공간 모델]
문헌 -> 벡터화
sequence of words -> verctor
단어와 단어 사이의 독립성
sequence에 의한 문맥 정보 손실
bag of words
웹 사이트 방문 같은 경우는 sequnce가 있음
	- sequnce labling
	- 주로 HMM(hidden marcov model)이나 CRF, ME(maximum entropy) 사용
데이터 마이닝에서는 대부분 vector (matrix)
	- feature 별로 값을 저장
	- feature 끼리는 독립적일 수록 좋음
word to vec
	- 단어마다 벡터를 가짐
	- 여전히 sequnce 정보는 담지 못함
[ppt] 문서는 벡터로 표현되며, 각각의 차원은 개별 단어에 대응됨 (??)
	- 각각의 단어를 통계학에서 demension
	- 머신러닝에서는 feature
	- 텍스트 마이닝에서는 unique wor (= vocabulary term)라고 부름
	- high-demensional space
		- 문헌 양이 많을수록 unique한 단어가 많아짐
	- curse of dimensionality
		- 고차원일수록 인지하기 힘듦
		- deimension reduction을 통해 2~3 demension으로 줄여서 이해하기 쉽게 함
		- 예: 통계학의 PCA(principle component analysys) 기법
		- 예: 관심있으면 TSNE 찾아볼 것
[ppt] 만약 문서 내에 특정 단어가 포함되어 있다면, 벡터 내에서 해당 차원은 0이 아닌 값을 갖게 됨
	- 문헌 안에 단어의 빈도수가 0이면 존재하지 않는 것
	- 대다수가 0이라 sparse한 matrix가 됨
[단어 가중치]
TF-IDF(TF=빈도, IDF=Inverse Document Frequency)
	- 단어가 문헌에 출현하는 횟수
	- 어떤 단어가 모든 문헌에 나타난다면 문헌과 문헌을 구분하는 능력이 떨어짐
	- 특정한 문헌에 특징되어 나타나는 단어 == 그 문헌을 잘 대변해준다
	- 가중치를 통해 문헌들이 더 잘 분류되도록 함
[벡터 공간 모델의 응용]
문서 벡터
	- 색인어(=feature, vocabulary term)
문헌*단어 매트릭스
	- 문서 벡터들을 모아 행렬로 만듦
[벡터 공간 모델의 한계]
긴 문헌들이 더 좋은 값을 가지므로 normalization 과정 필요
[문헌*단어 매트릭스 생성]
메모리가 많이 필요. 부족하면 매트릭스 생성 불가
lucene.apache.org
	- 오픈소스 검색 엔진
	- 몇천만건의 문헌 정도는 충분히 다룰 수 있음
	- 인덱싱과 서칭 가능
VectorSpaceModelManager.java
[단어 가중치 기법]
Zipf's law
	- power loaw distribution
	- 고빈도 단어 대부분은 쓸모없는 단어(불용어, 기능어, ...)
	- 문헌 식별에 큰 도움 되지 않음
TF-IDF
	- 일반적인 단어들은 이 값이 떨어짐
	- 중요도 측정
단어 빈도
	- 빈도수는 문헌이 길면 증가될 확률 높음
	- 그래서 TF를 계산
	- IDF는 로그 씌워야 함. 피피티 틀림
엔트로피
	- 엔트로피가 작다 -> 불확실성이 작다 -> 가치가 떨어짐
카이제곱
KL-divergence (관심있으면 찾아보자)
[정보 검색]
[단어 가중치 기법]
TF-IDF
	- 한 번 스캔해서 inverted index 만듦 (키: 단어, 벨류: 인덱스)
	- HashMap<String, HashSet<Integer>> invertedIndex = new HashMap();
	- 각 단어가 어떤 어떤 문헌에 나타났는지 알 수 있음. ArrayList를 쓰면 duplicate 존재해서 안됨
